{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/kiraliang/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../conversations/dsat/voip-0f41c16f2f-20130325_192310.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (data_path) as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SYSTEM] Hello , welcome to the Cambridge restaurant system? You can ask for restaurants by area , price range or food type . How may I help you?\n",
      "[USER]   i'm looking for a restaurant that serves german food\n",
      "[SYSTEM] You are looking for a german restaurant right?\n",
      "[USER]   no\n",
      "[SYSTEM] Let me confirm , You are looking for a restaurant in the moderate price range right?\n",
      "[USER]   yes\n",
      "[SYSTEM] I'm sorry but there is no restaurant serving german food\n",
      "[USER]   how about free\n",
      "[SYSTEM] I'm sorry but there is no restaurant serving german food\n",
      "[USER]   a restaurant serving french food\n",
      "[SYSTEM] You are looking for a german restaurant right?\n",
      "[USER]   thank you goodbye\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    with open (file) as f:\n",
    "        lines = [line.rstrip('\\n') for line in f]\n",
    "    id_name = file.split(\"/\")[-1].split(\".txt\")[0]\n",
    "    return id_name,lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_repetition(lines):\n",
    "    new_list =[]\n",
    "    for line in lines:\n",
    "        if \"SYSTEM\" in line:\n",
    "            new_list.append(line.strip())\n",
    "    #create dictionary and count the repetition of sentence (Feature 1)\n",
    "    new_dict = {}\n",
    "    for li in new_list:\n",
    "        if li in new_dict:\n",
    "            new_dict[li] += 1\n",
    "        else:\n",
    "            new_dict[li] = 1\n",
    "    num_rep = sum([value for value in new_dict.values() if value > 1])\n",
    "    #Percentage of the repetition sentence in the entire conversation (Feature 3)\n",
    "    num_rep_per = num_rep/len(new_list)\n",
    "    \n",
    "    return num_rep,num_rep_per, len(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_neg_conv(file):\n",
    "    # get everything\n",
    "    \n",
    "    dict_conv_only = create_user_conv(file)\n",
    "    compound_score, tot_pos_sen, tot_neg_sen = get_sentiment(dict_conv_only)\n",
    "    return compound_score, tot_pos_sen, tot_neg_sen\n",
    "    \n",
    "    \n",
    "def get_sentiment(dict_conv_only):\n",
    "    # generate the compound sentiment score and store in conversation\n",
    "\n",
    "    for sentence in dict_conv_only.keys():\n",
    "        compound_sentence = generatesentiment(sentence)\n",
    "        # add compound score to sentence \n",
    "        dict_conv_only[sentence] = compound_sentence\n",
    "    # add total compound score of sentence to conversation\n",
    "    compound_score = sum(dict_conv_only.values())/len(dict_conv_only.values())\n",
    "    tot_pos_sen = len([x for x in dict_conv_only.values() if x > 0])/len(dict_conv_only.values())\n",
    "    tot_neg_sen = len([x for x in dict_conv_only.values() if x < 0])/len(dict_conv_only.values())\n",
    "    return compound_score, tot_pos_sen, tot_neg_sen\n",
    "    \n",
    "    \n",
    "def create_user_conv(lines):\n",
    "    #create a list per conversation of only USER sentences \n",
    "    # and 'thankyou goodbye left out'\n",
    "    conv_only_user = []\n",
    "    new_list =[]\n",
    "    for line in lines:\n",
    "        # only USER input\n",
    "        if \"USER\" in line:\n",
    "            # Remove the word '[USER]'\n",
    "            line = line.replace(\"[USER]\", \"\")\n",
    "            line = line.strip()\n",
    "            new_list.append(line)\n",
    "    # Remove 'thank you good bye' if in last sentences\n",
    "    for i,j in dic.items():\n",
    "        new_list[-1] = new_list[-1].replace(i, j)\n",
    "        new_list[-2] = new_list[-2].replace(i, j)\n",
    "    conv_only_user.append(new_list)\n",
    "\n",
    "    # convert to dict \n",
    "    # with every conversation a dict with sentences as keys\n",
    "    # and compound pos neg score (as 0)\n",
    "    for conv in conv_only_user:\n",
    "        new_dict2 = {}\n",
    "        for sentence in conv:\n",
    "            new_dict2[sentence] = 0\n",
    "    return new_dict2       \n",
    "        \n",
    "def generatesentiment(sentence):\n",
    "    # function for sentiment analysis on sentences\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    return ss['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repetition of the chatbot / 4 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def number_repetition(file):\n",
    "#     with open (file) as f:\n",
    "#         lines = f.readlines()\n",
    "#         new_list =[]\n",
    "#         for line in lines:\n",
    "#             if \"SYSTEM\" in line:\n",
    "#                 new_list.append(line.strip())\n",
    "#     #create dictionary and count the repetition of sentence (Feature 1)\n",
    "#     new_dict = {}\n",
    "#     for li in new_list:\n",
    "#         if li in new_dict:\n",
    "#             new_dict[li] += 1\n",
    "#         else:\n",
    "#             new_dict[li] = 1\n",
    "#     num_rep = sum([value for value in new_dict.values() if value > 1])\n",
    "    \n",
    "#     #create a matrix to calculate repetition (Feature 2)\n",
    "#     m = [[0]* len(new_list) for i in range(len(new_list))]\n",
    "#     for i, x in enumerate(new_list):\n",
    "#         for j, y in enumerate(new_list):\n",
    "#             if x == y:\n",
    "#                 m[i][j] = 1 \n",
    "#     all_value = 0\n",
    "#     for li in m:\n",
    "#         a = sum(li)\n",
    "#         all_value+=a\n",
    "#     num_rep_exp = (all_value-len(new_list))/2\n",
    "    \n",
    "#     #Percentage of the repetition sentence in the entire conversation (Feature 3)\n",
    "#     num_rep_per = num_rep/len(new_list)\n",
    "#     #Number of sentences in the conversation (Feature 4)\n",
    "#     len_conversation = len(lines)\n",
    "    \n",
    "#     return num_rep, num_rep_per,num_rep_exp,len_conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "text_files_dsat = glob.glob(\"../conversations/dsat/*.txt\")\n",
    "text_files_sat = glob.glob(\"../conversations/sat/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences to delete\n",
    "dic = {'thankyou':\"\", 'thank you':\"\", \"goodbye\":\"\", \"good bye\":\"\", 'bye':\"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SYSTEM] Hello , welcome to the Cambridge restaurant system? You can ask for restaurants by area , price range or food type . How may I help you?\n",
      "[USER]   yes can i have a restaurant who\n",
      "[SYSTEM] What kind of food would you like?\n",
      "[USER]   cheap\n",
      "[SYSTEM] What kind of food would you like?\n",
      "[USER]   european\n",
      "[SYSTEM] Sorry there is no european restaurant in the cheap price range\n",
      "[USER]   european food any price\n",
      "[SYSTEM] Sorry there is no european restaurant in the cheap price range\n",
      "[USER]   okay can i have european food\n",
      "[SYSTEM] Sorry there is no european restaurant in the cheap price range\n",
      "[USER]   any price range\n",
      "[SYSTEM] Sorry there is no european restaurant in the cheap price range\n",
      "[USER]   uh european food\n",
      "[SYSTEM] Sorry there is no european restaurant in the cheap price range\n",
      "[USER]   can i have european food\n",
      "[SYSTEM] Sorry there is no european restaurant in the cheap price range\n",
      "[USER]   can i have the in any price range\n",
      "[SYSTEM] Sorry there is no european restaurant in the cheap price range\n",
      "[USER]   and its in the price range\n",
      "[SYSTEM] There are  restaurants serving european in the cheap price range . What area would you like?\n",
      "[USER]   don't care\n",
      "[SYSTEM] Sorry there is no european restaurant in the cheap price range\n",
      "[USER]   any price range\n",
      "[SYSTEM] Sorry there is no european restaurant in the cheap price range\n",
      "[USER]   any price range\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(text_files_dsat[6]) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create feature dataframe for disatisfied dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['id','num_rep','num_rep_per','len_conversation','total_compound_conv', 'tot_pos_sen', 'tot_neg_sen', 'Is_satisfied']\n",
    "df_rep_dsat = pd.DataFrame(index=range(len(text_files_dsat)), columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,file in enumerate(text_files_dsat):\n",
    "    id_number,lines = read_file(file)\n",
    "    num_rep, num_rep_per,len_conversation = number_repetition(lines)\n",
    "    compound_score, tot_pos_sen, tot_neg_sen = pos_neg_conv(lines)\n",
    "    df_rep_dsat.iloc[idx] = pd.Series({'id':id_number, 'num_rep':num_rep, 'num_rep_per':num_rep_per,\n",
    "                                  'len_conversation':len_conversation,\n",
    "                                       'total_compound_conv':compound_score, 'tot_pos_sen':tot_pos_sen, \n",
    "                                       'tot_neg_sen':tot_neg_sen, 'Is_satisfied':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>num_rep</th>\n",
       "      <th>num_rep_per</th>\n",
       "      <th>len_conversation</th>\n",
       "      <th>total_compound_conv</th>\n",
       "      <th>tot_pos_sen</th>\n",
       "      <th>tot_neg_sen</th>\n",
       "      <th>Is_satisfied</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>voip-dda7c88c6e-20130323_053612</td>\n",
       "      <td>2</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>28</td>\n",
       "      <td>-0.0134429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>voip-31de0daa7b-20130401_204621</td>\n",
       "      <td>11</td>\n",
       "      <td>0.55</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>voip-9819537952-20130327_023510</td>\n",
       "      <td>5</td>\n",
       "      <td>0.625</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0206286</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>voip-e61fa89add-20130326_004919</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0754333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>voip-7e07d8f0f5-20130328_190516</td>\n",
       "      <td>8</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0557667</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id num_rep num_rep_per len_conversation  \\\n",
       "0  voip-dda7c88c6e-20130323_053612       2    0.142857               28   \n",
       "1  voip-31de0daa7b-20130401_204621      11        0.55               40   \n",
       "2  voip-9819537952-20130327_023510       5       0.625               16   \n",
       "3  voip-e61fa89add-20130326_004919       0           0                6   \n",
       "4  voip-7e07d8f0f5-20130328_190516       8    0.571429               28   \n",
       "\n",
       "  total_compound_conv tot_pos_sen tot_neg_sen Is_satisfied  \n",
       "0          -0.0134429    0.142857    0.142857            0  \n",
       "1                   0           0           0            0  \n",
       "2           0.0206286    0.142857    0.142857            0  \n",
       "3           0.0754333    0.333333           0            0  \n",
       "4           0.0557667        0.25           0            0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rep_dsat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create feature dataframe for satisfied dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['id','num_rep','num_rep_per','len_conversation','total_compound_conv', 'tot_pos_sen', 'tot_neg_sen', 'Is_satisfied']\n",
    "df_rep_sat = pd.DataFrame(index=range(len(text_files_sat)), columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,file in enumerate(text_files_sat):\n",
    "    index,lines = read_file(file)\n",
    "    num_rep, num_rep_per,len_conversation = number_repetition(lines)\n",
    "    compound_score, tot_pos_sen, tot_neg_sen = pos_neg_conv(lines)\n",
    "    df_rep_sat.iloc[idx] = pd.Series({'id':index,'num_rep':num_rep, 'num_rep_per':num_rep_per,\n",
    "                                  'len_conversation':len_conversation,\n",
    "                                       'total_compound_conv':compound_score, 'tot_pos_sen':tot_pos_sen, \n",
    "                                       'tot_neg_sen':tot_neg_sen, 'Is_satisfied':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>num_rep</th>\n",
       "      <th>num_rep_per</th>\n",
       "      <th>len_conversation</th>\n",
       "      <th>total_compound_conv</th>\n",
       "      <th>tot_pos_sen</th>\n",
       "      <th>tot_neg_sen</th>\n",
       "      <th>Is_satisfied</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>voip-22c938c8ba-20130325_130445</td>\n",
       "      <td>8</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>28</td>\n",
       "      <td>-0.0104083</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>voip-9f989824fd-20130325_204229</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.07985</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>voip-bde2721237-20130326_200505</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01544</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>voip-a7ddefaeb3-20130328_173142</td>\n",
       "      <td>18</td>\n",
       "      <td>0.72</td>\n",
       "      <td>50</td>\n",
       "      <td>0.121847</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.0526316</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>voip-db80a9e6df-20130328_230211</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0505833</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id num_rep num_rep_per len_conversation  \\\n",
       "0  voip-22c938c8ba-20130325_130445       8    0.571429               28   \n",
       "1  voip-9f989824fd-20130325_204229       0           0               16   \n",
       "2  voip-bde2721237-20130326_200505       0           0               10   \n",
       "3  voip-a7ddefaeb3-20130328_173142      18        0.72               50   \n",
       "4  voip-db80a9e6df-20130328_230211       0           0               12   \n",
       "\n",
       "  total_compound_conv tot_pos_sen tot_neg_sen Is_satisfied  \n",
       "0          -0.0104083        0.25        0.25            1  \n",
       "1             0.07985    0.333333           0            1  \n",
       "2             0.01544         0.2           0            1  \n",
       "3            0.121847    0.368421   0.0526316            1  \n",
       "4           0.0505833    0.333333           0            1  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rep_sat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of disatisfied files: 157\n",
      "Number of satisfied files: 314\n"
     ]
    }
   ],
   "source": [
    "print('Number of disatisfied files: {}'.format(len(df_rep_dsat)))\n",
    "print('Number of satisfied files: {}'.format(len(df_rep_sat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_rep_dsat)/(len(df_rep_dsat)+len(df_rep_sat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_rep_sat)/(len(df_rep_dsat)+len(df_rep_sat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concat 2 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([df_rep_dsat,df_rep_sat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "features = pd.read_csv(\"features_tfidf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>num_rep</th>\n",
       "      <th>num_rep_per</th>\n",
       "      <th>len_conversation</th>\n",
       "      <th>total_compound_conv</th>\n",
       "      <th>tot_pos_sen</th>\n",
       "      <th>tot_neg_sen</th>\n",
       "      <th>sum_tfidf</th>\n",
       "      <th>avg_tfidf</th>\n",
       "      <th>Is_satisfied</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>voip-dda7c88c6e-20130323_053612</td>\n",
       "      <td>2</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>28</td>\n",
       "      <td>-0.013443</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>5.799495</td>\n",
       "      <td>0.087871</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>voip-31de0daa7b-20130401_204621</td>\n",
       "      <td>11</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.623657</td>\n",
       "      <td>0.068371</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>voip-9819537952-20130327_023510</td>\n",
       "      <td>5</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>16</td>\n",
       "      <td>0.020629</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>2.821653</td>\n",
       "      <td>0.100773</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>voip-e61fa89add-20130326_004919</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.075433</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.311470</td>\n",
       "      <td>0.122647</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>voip-7e07d8f0f5-20130328_190516</td>\n",
       "      <td>8</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>28</td>\n",
       "      <td>0.055767</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.426133</td>\n",
       "      <td>0.085118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id  num_rep  num_rep_per  len_conversation  \\\n",
       "0  voip-dda7c88c6e-20130323_053612        2     0.142857                28   \n",
       "1  voip-31de0daa7b-20130401_204621       11     0.550000                40   \n",
       "2  voip-9819537952-20130327_023510        5     0.625000                16   \n",
       "3  voip-e61fa89add-20130326_004919        0     0.000000                 6   \n",
       "4  voip-7e07d8f0f5-20130328_190516        8     0.571429                28   \n",
       "\n",
       "   total_compound_conv  tot_pos_sen  tot_neg_sen  sum_tfidf  avg_tfidf  \\\n",
       "0            -0.013443     0.142857     0.142857   5.799495   0.087871   \n",
       "1             0.000000     0.000000     0.000000   3.623657   0.068371   \n",
       "2             0.020629     0.142857     0.142857   2.821653   0.100773   \n",
       "3             0.075433     0.333333     0.000000   3.311470   0.122647   \n",
       "4             0.055767     0.250000     0.000000   4.426133   0.085118   \n",
       "\n",
       "   Is_satisfied  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### oversampling - SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    314\n",
       "0    157\n",
       "Name: Is_satisfied, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['Is_satisfied'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = features.iloc[:,1:-1]\n",
    "y = features.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After OverSampling, counts of label '1': 234\n",
      "After OverSampling, counts of label '0': 234\n"
     ]
    }
   ],
   "source": [
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train the data with oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8611111111111112, 0.775, 0.8157894736842106, None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_test, y_pred, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7627118644067796"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28 10]\n",
      " [18 62]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train the data without oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred_new = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_new = classifier.predict_proba(X_test)\n",
    "#y_pred_new2 = classifier.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8409090909090909, 0.925, 0.8809523809523809, None)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(y_test, y_pred_new, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8305084745762712"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tune the parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=3,\n",
       "       param_grid={'C': array([1., 2., 3., 4., 5.]), 'solver': ['newton-cg', 'lbfgs', 'liblinear'], 'class_weight': [None, 'balanced']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'C': np.linspace(1, 5, 5),\n",
    "    'solver':['newton-cg','lbfgs','liblinear'],\n",
    "    'class_weight':[None,'balanced']\n",
    "             }\n",
    "lr = LogisticRegression(random_state=0)\n",
    "clf = GridSearchCV(lr, parameters, cv=5,n_jobs=3)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 4.0, 'class_weight': None, 'solver': 'liblinear'}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=3,\n",
       "       param_grid={'penalty': ['l1', 'l2']}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(lr, {'penalty':['l1','l2']}, cv=5,n_jobs=3)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'penalty': 'l2'}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train the model again with optimized parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=4,penalty='l2',solver='liblinear',random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_opt = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8390804597701149, 0.9125, 0.8742514970059879, None)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(y_test, y_pred_opt, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8220338983050848"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### though gridsearch suggested C=4, but after training the model, the accuracy is higher to set C=1\n",
    "C: Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8305084745762712"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1,penalty='l2',solver='liblinear',random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_final = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coefficient of the features in the decision function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.19954161, -1.22673667, -0.018489  , -0.71131447,  0.65721879,\n",
       "        -0.01281874,  0.5190334 , -0.38626585]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intercept (a.k.a. bias) added to the decision function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10979743])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num_rep               -0.477934\n",
       "num_rep_per           -0.522149\n",
       "len_conversation      -0.353808\n",
       "total_compound_conv   -0.098092\n",
       "tot_pos_sen            0.064181\n",
       "tot_neg_sen           -0.022449\n",
       "sum_tfidf              0.140452\n",
       "avg_tfidf              0.026714\n",
       "dtype: float64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.iloc[:,1:].drop(\"Is_satisfied\", axis=1).apply(lambda x: x.corr(features.Is_satisfied))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import *\n",
    "import numpy as np\n",
    "from scipy.stats.stats import pearsonr\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            PCC        p-value\n",
      "avg_tfidf__Is_satisfied                0.026714   5.630495e-01\n",
      "len_conversation__Is_satisfied        -0.353808   2.460829e-15\n",
      "len_conversation__avg_tfidf           -0.550678   1.059853e-38\n",
      "len_conversation__sum_tfidf            0.320102   1.105667e-12\n",
      "len_conversation__tot_neg_sen          0.117055   1.100980e-02\n",
      "len_conversation__tot_pos_sen          0.066764   1.479761e-01\n",
      "len_conversation__total_compound_conv  0.141350   2.105281e-03\n",
      "num_rep__Is_satisfied                 -0.477934   2.981528e-28\n",
      "num_rep__avg_tfidf                    -0.354889   1.997778e-15\n",
      "num_rep__len_conversation              0.858641  3.229487e-138\n",
      "num_rep__num_rep_per                   0.856732  5.911338e-137\n",
      "num_rep__sum_tfidf                     0.011395   8.051803e-01\n",
      "num_rep__tot_neg_sen                   0.083387   7.060020e-02\n",
      "num_rep__tot_pos_sen                   0.077346   9.361157e-02\n",
      "num_rep__total_compound_conv           0.168678   2.356180e-04\n",
      "num_rep_per__Is_satisfied             -0.522149   2.668523e-34\n",
      "num_rep_per__avg_tfidf                -0.157658   5.947938e-04\n",
      "num_rep_per__len_conversation          0.592383   5.972811e-46\n",
      "num_rep_per__sum_tfidf                -0.134577   3.431100e-03\n",
      "num_rep_per__tot_neg_sen               0.048848   2.900849e-01\n",
      "num_rep_per__tot_pos_sen               0.040176   3.843204e-01\n",
      "num_rep_per__total_compound_conv       0.172967   1.617397e-04\n",
      "sum_tfidf__Is_satisfied                0.140452   2.248938e-03\n",
      "sum_tfidf__avg_tfidf                  -0.140158   2.297880e-03\n",
      "tot_neg_sen__Is_satisfied             -0.022449   6.269947e-01\n",
      "tot_neg_sen__avg_tfidf                -0.052980   2.511578e-01\n",
      "tot_neg_sen__sum_tfidf                 0.135895   3.125137e-03\n",
      "tot_pos_sen__Is_satisfied              0.064181   1.643388e-01\n",
      "tot_pos_sen__avg_tfidf                -0.100144   2.977464e-02\n",
      "tot_pos_sen__sum_tfidf                 0.062943   1.726466e-01\n",
      "tot_pos_sen__tot_neg_sen              -0.203908   8.176718e-06\n",
      "total_compound_conv__Is_satisfied     -0.098092   3.330986e-02\n",
      "total_compound_conv__avg_tfidf        -0.093656   4.219115e-02\n",
      "total_compound_conv__sum_tfidf         0.069850   1.300927e-01\n",
      "total_compound_conv__tot_neg_sen      -0.457296   1.022148e-25\n",
      "total_compound_conv__tot_pos_sen       0.788434  4.907513e-101\n"
     ]
    }
   ],
   "source": [
    "correlations = {}\n",
    "columns = features.columns[1:].tolist()\n",
    "\n",
    "for col_a, col_b in itertools.combinations(columns, 2):\n",
    "    correlations[col_a + '__' + col_b] = pearsonr(features.loc[:, col_a], features.loc[:, col_b])\n",
    "\n",
    "result = DataFrame.from_dict(correlations, orient='index')\n",
    "result.columns = ['PCC', 'p-value']\n",
    "\n",
    "print(result.sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model with one variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_shuffle.iloc[:,:1]\n",
    "y = df_shuffle.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of num_rep: 0.70\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of num_rep: {:.2f}'.format(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of num_rep_per: 0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X = df_shuffle.iloc[:,1:2]\n",
    "y = df_shuffle.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')\n",
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "print('Accuracy of num_rep_per: {:.2f}'.format(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of num_rep_exp: 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X = df_shuffle.iloc[:,2:3]\n",
    "y = df_shuffle.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')\n",
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "print('Accuracy of num_rep_exp: {:.2f}'.format(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of len_conversation: 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X = df_shuffle.iloc[:,3:4]\n",
    "y = df_shuffle.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')\n",
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "print('Accuracy of len_conversation: {:.2f}'.format(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
